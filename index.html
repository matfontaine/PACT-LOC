<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>PACT - Module Localisation</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/telecom.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/github.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section class="cover" data-background=""  data-state="no-title-footer no-progressbar has-dark-background">

					<h2 id='coverh2'>Module Localisation de Sources sonores</h2>
					<h1  id='title_seminar'> PACT</h1>
					<h3><a href="https://matfontaine.github.io/PACT-MFCC", id='github_url'>matfontaine.github.io/PACT-LOC</a></h3>
					<p id='coverauthors'>
						Mathieu FONTAINE<br />
						mathieu.fontaine@telecom-paris.fr
					</p>
					<p id="date">
					2022-2023</br>
					</p>
					<p>
					<img src="css/theme/img/logo-Telecom.svg" id="telecom" class="logo" alt="">
					<aside class="notes">
						<ul><li>We will consider historical audio source separation technique</li>
									<li>e.g. no deep learning extensions or nonnegative matrix factorization</li>
								<li>the Handbook for that course is available on the moodle (PAM/Audio_source_separation)</li>
						</ul>
					</aside>
				</section>

				<!-- Outline of the presentation -->
				<section>
				<h1>Attentes du module</h1>
				<ul>
					<li>Comprendre l'algorithme de localisation MUSIC</li>
					<li>Implémentation à la main de MUSIC sous Python</li>
					<li>Application au projet PACT (pourquoi ce module est adapté etc.)</li>
					<li>Résultats avec la méthode implémentée + comparaison avec version de <a href ="https://pyroomacoustics.readthedocs.io/en/pypi-release/">Pyroomacoustics</a></li>
				</ul>

				<h2>Livrable</h2>
				<ul>
					<li>Code de votre groupe en Python avec quelques résultats.</li>
				</ul>
</section>

<section>
	<h1>Plan</h1>
	<h2>I - Transformée de Fourier à court terme </h2>
	<h2>II - Modèle et hypothèse</h2>
	<h2>III - Algorithme MUSIC</h2>
</section>
<section class="cover" data-background="" data-state="no-title-footer no-progressbar has-dark-background">
	<h2 id='coverh2'>I - Transformée de Fourier à court terme</h2>

</section>
<section>
	<h2>La transformée de Fourier discrète</h2>
Soit un signal discret $x_0,\dots,x_{N-1}$. Sa transformée de Fourier (discrète) est:

<center>$$
X(f) = \sum_{n=0}^{N-1}x_ne^{-i\frac{2\pi f n}{N}}
$$
</center>
<ul>
	<li>On obtient une représentation fréquentielle du signal</li>
	<li>Complexité algorithmique : $O(N^2)$</li>
</ul></br>
Il existe une version édulcorée qui réduit la complexité $O(N^2)$ en $O(N\log(N))\\$ (cf. <a href ="https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm">Cooley-Tukey FFT algorithm</a>)</br>

<center><figure>
<img src="figures/images/fft.jpg" width="65%">
<figcaption><em>Figure : champ d'une Baleine. On remarque l'activité dans les basses fréquences.</em></figcaption>
</figure>
</center>
<p>
<div class="remarque">On emploiera la FFT dans la suite.</div>
</p>
</section>


<section>
	<h2>Transformée de Fourier à Court Terme (TFCT)</h2>
	<ul>
		<li>Découpage d'un signal en plusieurs séquences</li>
		<li>multiplication par un fenêtrage (Pourquoi ? <em>cf.</em> <a href="https://perso.telecom-paristech.fr/ladjal/PDFS/OASIS.pdf">poly OASIS</a>) </li>
		<li>FFT sur chacune des séquences</li>
	</ul>
	<center><figure>
		<img src="figures/images/STFT.png" width="50%">
		<figcaption><em>TFCT d'un signal de parole (selon Laroche)</em></figcaption>
		</figure>
		</center>	
 <div class="remarque">On obtient un signal "temps-fréquence" $X(f,t)$</div>
</section>

<section>
<h2>Spectrogramme de puissance</h2>
<ul><li>Module au carré des coefficients</li>
<center>$$
	S(f,t) = |X(f,t)|^2 \qquad\qquad\qquad \texttt{(Spectrogramme de Puissance)}
	$$
</center>
<li>Pour une meilleure représentation, on calcule le log du SP.</li>
</ul>
<center><figure>
	<img src="figures/background.png" width="70%">
	<figcaption><em>Figure : Log-Spectrogramme d'un signal de parole.</em></figcaption>
	</figure>
	</center>
</section>


<section class="cover" data-background="" data-state="no-title-footer no-progressbar has-dark-background">
	<h2 id='coverh2'>II - Modèle et hypothèse</h2>

</section>

<section>
	<h2>Position du problème</h2>
	<center><figure>
		<img src="figures/images/loc_problem.png" width="95%">
		</figure>
		</center>
</section>
<section>
<h1>Propagation du son (champ proche)</h1>
L'explication ici est extrêmement simplifié par rapport à un vrai cours d'acoustique (mais sera suffisant pour implémenter un algorithme de localisation)
<ul>
	<li>quand la source n'est pas loin des microphones (moins de 50cm pour des micros espacées de 5-10cm)</li>
	 <li>alors la propagation du son est représentée par des ondes sphériques.</li>
	</li>
</ul>

Si on considère $M$ microphones aux points $m_1, \dots, m_M$ et $p_1, \dots, p_L$ $L$ points avec ses coordonnées cartésiennes
 et $r_{ml}$ la distance euclidienne entre le point $a_l$ et le $n$-ème microphone
alors les vecteurs de pointage (représentant la propagation de la source au microphone) est un vecteur $\bold{a}_l(f) \in \mathbb{C}^M$ donnée par :
<center>
$$
\bold{a}_{l}(f) = [\frac{1}{r_{1l}}e^{-i\frac{\omega_f r_{1l}}{c_0}}, \dots, \frac{1}{r_{Ml}}e^{-i\frac{\omega_f r_{Ml}}{c_0}}]
$$
</center>
<ul>
<li>$\omega_f = 2\pi f$ : fréquence angulaire (avec $f$ en Hz)</li>
<li>$c_0 \approx 331$ m.s${^{-1}}$ : célérité du son dans l'air sec avec une température à 20°C.</li>
</ul>
</section>

<section>
	<h1>Propagation du son (champ lointain)</h1>
	<ul>
		<li>quand la source est plus loin des microphones (plus de 50cm pour des micros espacées de 5-10cm)</li>
		 <li>alors la propagation du son est représentée par des ondes planes et les sources peuvent être supposés être à l'infini</li>
		 <li>On estimera juste des angles d'arrivées (Azimuth et élévation) et non des distances</li>
	</ul>
	Si on considère $M$ microphones aux points $m_1, \dots, m_M$ et $\overrightarrow{u}_1, \dots, \overrightarrow{u}_L$ $L$ vecteurs unitaires orientés dans la direction de point lointain $L$
	alors le vecteur de pointage $\bold{a}_l(f) \in \mathbb{C}^M$ est donnée par :
	<center>
	$$
	\bold{a}_{l}(f) = [e^{-i k_f(l).m_1}, \dots, e^{-i k_f(l).m_M}]
	$$
	</center>
	<ul>
	<li>$k_f(l) = 2\pi f / c_0 \overrightarrow{u}_l$ : le vecteur d'onde</li>
	<li>$.$ dans l'exponentielle représente ici le produit scalaire</li>
	<li>Les $\overrightarrow{u}_l$ sont par exemple des coordonnées sphériques (si on prend l'azimuth et élévation) ou des coordonnées circulaires avec un rayon de $1$</li>
	</ul>
	</section>

<section>
<h2>Modèle acoustique</h2>
<ul>
	<li>Soient $s_1,\dots,s_N$  $N$ sources ponctuelles que l'on souhaite localiser</li>
	
	<li>on suppose la TFCT effectuée sur ces sources et on note $\bold{s}_{ft}= [s_{1ft}, \dots, s_{Nft}]^\top \in \mathbb{C}^M$ le vecteur contenant les sources ponctuelles au point temps-fréquence $f,t$
	<li>Soit $\bold{x}_{ft}= [x_{1ft}, \dots, x_{Nft}]^\top \in \mathbb{C}^M$ l'acquisition du signal par $M$ microphones</li>
	<li>On définit également le vecteur $\bold{n}_{ft}=[n_{1ft}, \dots, n_{Mft}]^\top \in \mathbb{C}^M$ qui est un bruit additif parasitant l'acquisition des $M$ microphones, indépendante du signal</li>
</ul>

Un modèle classique (avec une réverbération "raisonnable") est de supposer le modèle de mélange suivant : 
<center>
$$
\bold{x}_{ft} = A_{f}\bold{s}_{ft} + \bold{n}_{ft}
$$
</center>
où $\bold{A}_f \in \mathbb{C}^{M \times N}$ est appelée matrice de mélange et a le rôle de représenter la "propagation acoustique" des sources au microphones. 
</section>


<section class="cover" data-background="" data-state="no-title-footer no-progressbar has-dark-background">
	<h2 id='coverh2'>III - MUSIC</h2>

</section>

<section>
	<h1>Multiple signal classification (MUSIC)</h1>
<ul>
	<li>Pour l'article au grand complet, c'est  <a href="https://msol.people.uic.edu/ECE531/papers/Multiple%20Emitter%20Location%20and%20Signal%20Parameter%20Estimation.pdf"> ici !</a></li>
	<li>Idée : faire une décomposition en valeurs propres de la matrice de covariance du mélange $R_{x, ft} = \mathbb{E}(\bold{x}_{ft}\bold{x}_{ft}^\top)$ et exploiter le fait que les espaces propres engendrant le bruit et la sources sont orthogonaux</li>
</ul>
D'après $\bold{x}_{ft} = A_{f}\bold{s}_{ft} + \bold{n}_{ft}$ on obtient (si $n$ est gaussien de covariance $\sigma^2 I_M$) : 

<center>
$$R_{x, ft} = \bold{A}_fR_{s, ft}\bold{A}_f^H + \sigma^2I_M $$
</center>
avec $R_{s,ft}$ la matrice de covariance de la source.

<div class="remarque">
Un estimateur de $R_{x, ft}$  (si les sources sont immobiles) est donnée par : $$\widehat{\mathbf{R}}_{\mathbf{x}, f}=T^{-1} \sum_{t=1}^T \mathbf{x}_{f t} \mathbf{x}_{f t}^{\mathrm{H}}$$
</div>

<b><center>Attention ! cette algorithme ne marche que quand $N \leq M$</center></b>
</section>

<section>
	<h1>Suite MUSIC</h1>
	On peut alors montrer (c'est la contribution de MUSIC) que : 
<ul>
	<li>$R_{x, ft}$ est de rang plein</li>
	<li>Les $N \leq M$ plus grandes valeurs propres de $R_{x, ft}$ engendrent l'espace propre du signal $\bold{Q}_{s,f} \in \mathbb{C}^{M \times N}$</li>
	<li>l'espace propre du bruit $\bold{Q}_{n,f} \in \mathbb{C}^{M \times (M-N)}$ est orthogonal  à $\bold{Q}_{s,f}$</li>
</ul>
En particulier, il est possible de montrer que si $\theta$ est l'angle d'arrivé d'une source alors $\bold{a}_{f}^H(\theta)q_{n,ft} = 0$ où $q_{n,ft}$ est un vecteur propre de l'espace propre du bruit.

La fonction suivante (appelé pseudo spectre) : 
<center>
$$
P_f^{\operatorname{MUSIC}}(\theta)=\frac{1}{\mathbf{a}_f^H(\theta) \mathbf{Q}_{\mathbf{n}, f t} \mathbf{Q}_{\mathbf{n}, f t}^H \mathbf{a}_f(\theta)}
$$
</center>

tend alors vers l'infini pour les directions de la source  $\implies$ on en déduit notre algorithme de localisation ! 

</section>

<section>
<h1>Résumé de l'algorithme</h1>
<ul>
	<li>Calculer $\widehat{\mathbf{R}}_{\mathbf{x}, f}$ et faire une décomposition en valeurs propres pour obtenir $\mathbf{Q}_{\mathbf{n}, f}$ à partir des $N-M$ plus petites valeurs propres</li>
<li>Calculer $\sum_f P_f^{\text {MUSIC }}\left(\theta_l\right)$ pour différents points $\left\{\theta_l\right\}_l$ et faire du "pick peaking"</li>
</ul>
<h2>Variante encore plus performante et simple à implémenter: Norm-MUSIC</h2>
<ul>
	<li>Pour le détail technique c'est <a href="https://core.ac.uk/download/pdf/53355242.pdf">ici !</a></li>
	<li>Remplacer $\sum_f P_f^{\text {MUSIC }}\left(\theta_l\right)$ par 
		$$P^{\text {NormMUSIC }}\left(\theta_l\right)=\sum_{f=1}^F \frac{P_f^{\mathrm{MUSIC}}\left(\theta_l\right)}{\max _{l^{\prime}=1, \ldots, L} P_f^{\text {MUSIC }}\left(\theta_{l^{\prime}}\right)}$$
		 et faire du "pick peaking"</li>
	<li>Permet en quelque sorte de normaliser par la fréquence qui contient le plus d'information sur la position</li>
</ul>
</section>


</div>
<div class='footer'>
	<img src="css/theme/img/logo-Telecom.svg" alt="Logo"/>
	<div id="middlebox">PACT - Module Loc</div>
	<ul>
	</ul>
</div>
			</div>

		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				controls: false,
				progress: true,
				history: true,
				center: false,
				slideNumber: true,
				minScale: 0.1,
				maxScale: 5,
				transition: 'none', //

				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math-katex/math-katex.js', async: true },
					{ src: 'plugin/reveald3/reveald3.js' },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>

	</body>

</html>